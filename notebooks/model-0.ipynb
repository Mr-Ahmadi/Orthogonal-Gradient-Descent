{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d20dcf39",
      "metadata": {},
      "source": [
        "# GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0b0bf9e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Task A] Epoch 1/3 — Loss: 0.5934\n",
            "[Task A] Epoch 2/3 — Loss: 0.1326\n",
            "[Task A] Epoch 3/3 — Loss: 0.1095\n",
            ">>> Task A test accuracy BEFORE Task B: 97.55%\n",
            "[Task B] Epoch 1/3 — Loss: 0.7293\n",
            "[Task B] Epoch 2/3 — Loss: 0.2079\n",
            "[Task B] Epoch 3/3 — Loss: 0.1673\n",
            ">>> Task A test accuracy AFTER  Task B:  0.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_labels = mnist_train.targets.numpy()\n",
        "test_labels  = mnist_test.targets.numpy()\n",
        "\n",
        "idx_A_train = np.where(train_labels < 5)[0].tolist()\n",
        "idx_B_train = np.where(train_labels >= 5)[0].tolist()\n",
        "idx_A_test  = np.where(test_labels  < 5)[0].tolist()\n",
        "\n",
        "loader_A_train = DataLoader(\n",
        "    Subset(mnist_train, idx_A_train),\n",
        "    batch_size=64, shuffle=True, drop_last=True\n",
        ")\n",
        "loader_B_train = DataLoader(\n",
        "    Subset(mnist_train, idx_B_train),\n",
        "    batch_size=64, shuffle=True, drop_last=True\n",
        ")\n",
        "loader_A_test  = DataLoader(\n",
        "    Subset(mnist_test, idx_A_test),\n",
        "    batch_size=256, shuffle=False, drop_last=False\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 100), nn.ReLU(),\n",
        "    nn.Linear(100, 100),    nn.ReLU(),\n",
        "    nn.Linear(100, 10),\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "def eval_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "num_epochs_A = 3\n",
        "for epoch in range(1, num_epochs_A + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x, y in loader_A_train:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "    avg_loss = running_loss / len(loader_A_train.dataset)\n",
        "    print(f\"[Task A] Epoch {epoch}/{num_epochs_A} — Loss: {avg_loss:.4f}\")\n",
        "\n",
        "acc_before = eval_accuracy(model, loader_A_test)\n",
        "print(f\">>> Task A test accuracy BEFORE Task B: {acc_before*100:.2f}%\")\n",
        "\n",
        "num_epochs_B = 3\n",
        "for epoch in range(1, num_epochs_B + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x, y in loader_B_train:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "    avg_loss = running_loss / len(loader_B_train.dataset)\n",
        "    print(f\"[Task B] Epoch {epoch}/{num_epochs_B} — Loss: {avg_loss:.4f}\")\n",
        "\n",
        "acc_after = eval_accuracy(model, loader_A_test)\n",
        "print(f\">>> Task A test accuracy AFTER  Task B:  {acc_after*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03860818",
      "metadata": {},
      "source": [
        "# OGD-GTL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "de5c9126",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "de5c9126",
        "outputId": "708474b6-f55b-47bc-844a-fe726f826843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Task A (digits 0-4)...\n",
            "[Task A] Epoch 1/3 — Loss: 1.9245\n",
            "[Task A] Epoch 2/3 — Loss: 0.9401\n",
            "[Task A] Epoch 3/3 — Loss: 0.4406\n",
            "Task A accuracy BEFORE Task B: 93.75%\n",
            "Stored 32 orthonormal directions from Task A.\n",
            "Training Task B (digits 5-9) with OGD projection...\n",
            "[Task B] Epoch 1/3 — Loss: 5.3471\n",
            "[Task B] Epoch 2/3 — Loss: 3.8714\n",
            "[Task B] Epoch 3/3 — Loss: 2.7151\n",
            "Task A accuracy AFTER Task B: 93.05%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "\n",
        "# Orthogonal Gradient Descent (OGD) Utilities\n",
        "def gram_schmidt(vectors, eps=1e-10):\n",
        "    \"\"\"Orthonormalize a list of gradient vectors via Gram-Schmidt.\"\"\"\n",
        "    ortho = []\n",
        "    for v in vectors:\n",
        "        w = v.clone()\n",
        "        for u in ortho:\n",
        "            w -= (u @ v) / (u @ u + eps) * u\n",
        "        norm = w.norm()\n",
        "        if norm > eps:\n",
        "            ortho.append(w / norm)\n",
        "    return ortho\n",
        "\n",
        "class OGD(Optimizer):\n",
        "    \"\"\"Orthogonal Gradient Descent optimizer.\"\"\"\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        defaults = dict(lr=lr)\n",
        "        super().__init__(params, defaults)\n",
        "        self.directions = []\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = closure() if closure is not None else None\n",
        "        grads = [p.grad.view(-1) for p in self.param_groups[0]['params'] if p.grad is not None]\n",
        "        if not grads:\n",
        "            return loss\n",
        "        g = torch.cat(grads)\n",
        "        if self.directions:\n",
        "            V = torch.stack(self.directions)\n",
        "            alphas = V.mv(g)\n",
        "            g = g - V.t().mv(alphas)\n",
        "        offset = 0\n",
        "        for p in self.param_groups[0]['params']:\n",
        "            if p.grad is None:\n",
        "                continue\n",
        "            numel = p.numel()\n",
        "            p.grad.copy_(g[offset:offset+numel].view_as(p))\n",
        "            offset += numel\n",
        "        lr = self.param_groups[0]['lr']\n",
        "        for p in self.param_groups[0]['params']:\n",
        "            if p.grad is not None:\n",
        "                p.add_(p.grad, alpha=-lr)\n",
        "        return loss\n",
        "\n",
        "    def store_directions(self, model, dataloader, device='cpu', max_samples=2000):\n",
        "        \"\"\"Compute and store orthonormal gradient directions from dataloader.\"\"\"\n",
        "        model.eval()\n",
        "        collected = []\n",
        "        seen = 0\n",
        "        for x, y in dataloader:\n",
        "            if seen >= max_samples:\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            model.zero_grad()\n",
        "            with torch.enable_grad():\n",
        "                logits = model(x)\n",
        "                true_logits = logits[torch.arange(len(y)), y].sum()\n",
        "                true_logits.backward()\n",
        "            vec = [p.grad.view(-1).detach().cpu() for p in model.parameters()]\n",
        "            collected.append(torch.cat(vec))\n",
        "            seen += x.size(0)\n",
        "        new_dirs = gram_schmidt(collected)\n",
        "        self.directions = gram_schmidt(self.directions + new_dirs)\n",
        "\n",
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "mnist_test  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "y_train = np.array(mnist_train.targets)\n",
        "y_test  = np.array(mnist_test.targets)\n",
        "idx_A_train = np.where(y_train < 5)[0]\n",
        "idx_B_train = np.where(y_train >= 5)[0]\n",
        "idx_A_test  = np.where(y_test  < 5)[0]\n",
        "\n",
        "loader_A_train = DataLoader(Subset(mnist_train, idx_A_train), batch_size=64, shuffle=True)\n",
        "loader_B_train = DataLoader(Subset(mnist_train, idx_B_train), batch_size=64, shuffle=True)\n",
        "loader_A_test  = DataLoader(Subset(mnist_test,  idx_A_test),  batch_size=256, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(28*28, 100), nn.ReLU(),\n",
        "    nn.Linear(100, 100),   nn.ReLU(),\n",
        "    nn.Linear(100, 10)\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = OGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def eval_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# === TRAIN Task A ===\n",
        "print(\"Training Task A (digits 0-4)...\")\n",
        "for epoch in range(1, 4):\n",
        "    model.train()\n",
        "    cum_loss = 0\n",
        "    for x, y in loader_A_train:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cum_loss += loss.item() * x.size(0)\n",
        "    print(f\"[Task A] Epoch {epoch}/3 — Loss: {cum_loss/len(loader_A_train.dataset):.4f}\")\n",
        "\n",
        "# Eval before Task B\n",
        "acc_before = eval_accuracy(model, loader_A_test)\n",
        "print(f\"Task A accuracy BEFORE Task B: {acc_before*100:.2f}%\")\n",
        "\n",
        "# Store Task A directions\n",
        "optimizer.store_directions(model, loader_A_train, device=device, max_samples=2000)\n",
        "print(f\"Stored {len(optimizer.directions)} orthonormal directions from Task A.\")\n",
        "\n",
        "# Adjust LR for Task B\n",
        "optimizer.param_groups[0]['lr'] = 1e-4\n",
        "\n",
        "# === TRAIN Task B without rehearsal ===\n",
        "print(\"Training Task B (digits 5-9) with OGD projection...\")\n",
        "for epoch in range(1, 4):\n",
        "    model.train()\n",
        "    cum_loss = 0\n",
        "    for x, y in loader_B_train:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cum_loss += loss.item() * x.size(0)\n",
        "    print(f\"[Task B] Epoch {epoch}/3 — Loss: {cum_loss/len(loader_B_train.dataset):.4f}\")\n",
        "\n",
        "# Eval after Task B\n",
        "acc_after = eval_accuracy(model, loader_A_test)\n",
        "print(f\"Task A accuracy AFTER Task B: {acc_after*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
